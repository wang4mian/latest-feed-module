// =====================================================
// Supabase Edge Function: AI ÂàÜÊûê
// Ë∑ØÂæÑ: supabase/functions/ai-analyze/index.ts  
// ÂäüËÉΩ: ‰ΩøÁî®Crawl4AIÂíåGemini AIÂàÜÊûêÊñáÁ´†ÂÜÖÂÆπ
// =====================================================

import { serve } from "https://deno.land/std@0.177.0/http/server.ts"
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
}

interface ArticleForAnalysis {
  id: string
  title: string
  link: string
  description: string
  source_id: number
  topic_for_ai: string
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders })
  }

  try {
    // Initialize Supabase client
    const supabaseUrl = Deno.env.get('SUPABASE_URL')!
    const supabaseKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
    const supabase = createClient(supabaseUrl, supabaseKey)

    // Get request parameters
    const { article_id, batch_size = 15 } = await req.json()

    let articles: ArticleForAnalysis[] = []

    if (article_id) {
      // Process specific article
      const { data, error } = await supabase
        .from('articles')
        .select(`
          id, title, link, description, source_id,
          rss_sources!inner(topic_for_ai)
        `)
        .eq('id', article_id)
        .eq('overall_status', 'draft')
        .limit(1)

      if (error) throw error
      articles = data?.map(a => ({
        ...a,
        topic_for_ai: a.rss_sources.topic_for_ai
      })) || []
    } else {
      // Process batch of articles
      const { data, error } = await supabase
        .from('articles')
        .select(`
          id, title, link, description, source_id,
          rss_sources!inner(topic_for_ai)
        `)
        .eq('overall_status', 'draft')
        .is('ai_score', null)
        .limit(batch_size)

      if (error) throw error
      articles = data?.map(a => ({
        ...a,
        topic_for_ai: a.rss_sources.topic_for_ai
      })) || []
    }

    if (!articles.length) {
      return new Response(JSON.stringify({
        success: true,
        message: 'No articles to process',
        processed: 0
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      })
    }

    let processed = 0
    const results = []

    // Process each article
    for (const article of articles) {
      try {
        console.log(`Processing article: ${article.title}`)

        // Step 1: Extract full content with Crawl4AI (with description fallback)
        const fullContent = await extractFullContent(article.link, article.description)
        
        // Step 2: AI Analysis with Gemini
        const aiAnalysis = await analyzeWithGemini(article, fullContent)

        // Step 3: Extract and store images separately
        let extractedImages = null
        try {
          if (fullContent.includes('[ÊñáÁ´†ÂõæÁâá‰ø°ÊÅØ]')) {
            const imageSection = fullContent.split('[ÊñáÁ´†ÂõæÁâá‰ø°ÊÅØ]:')[1]
            if (imageSection) {
              const imageUrls = imageSection.match(/(https?:\/\/[^\s\)]+)/g) || []
              if (imageUrls.length > 0) {
                extractedImages = imageUrls.map((url, index) => ({
                  url: url.trim(),
                  alt: `ÂõæÁâá${index + 1}`,
                  source: 'jina_ai'
                }))
                console.log(`üì∏ Extracted ${extractedImages.length} images for article`)
              }
            }
          }
        } catch (error) {
          console.error('Error extracting images:', error)
        }

        // Step 4: Update article with analysis results
        const { error: updateError } = await supabase
          .from('articles')
          .update({
            full_content: fullContent,
            ai_score: aiAnalysis.relevance_score,
            ai_reason: aiAnalysis.relevance_reason,
            ai_category: aiAnalysis.primary_category,
            ai_summary: aiAnalysis.summary_for_editor,
            ai_strategic_implication: aiAnalysis.strategic_implication,
            // Â≠òÂÇ®ÁºñËØëÂ•ΩÁöÑÂø´ËÆØÂà∞editor_notesÂ≠óÊÆµÔºå‰æõÁºñËæëÂ∑•‰ΩúÂè∞‰ΩøÁî®
            editor_notes: aiAnalysis.compiled_briefing || null,
            // Â≠òÂÇ®ÂõæÁâá‰ø°ÊÅØÂà∞crawl_metadataÂ≠óÊÆµ
            crawl_metadata: extractedImages ? { images: extractedImages } : null,
            overall_status: aiAnalysis.relevance_score >= 50 ? 'ready_for_review' : 'auto_rejected',
            updated_at: new Date().toISOString()
          })
          .eq('id', article.id)

        if (!updateError) {
          processed++
          
          // Step 4: Extract and store entities
          if (aiAnalysis.entities) {
            await processEntities(supabase, article.id, aiAnalysis.entities)
          }

          results.push({
            id: article.id,
            title: article.title,
            score: aiAnalysis.relevance_score,
            category: aiAnalysis.primary_category,
            status: 'processed'
          })
        }

      } catch (error) {
        console.error(`Error processing article ${article.id}:`, error)
        results.push({
          id: article.id,
          title: article.title,
          status: 'error',
          error: error.message
        })
      }
    }

    return new Response(JSON.stringify({
      success: true,
      message: 'AI analysis completed',
      timestamp: new Date().toISOString(),
      processed,
      total: articles.length,
      results
    }), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    })

  } catch (error) {
    console.error('AI Analysis failed:', error)
    
    return new Response(JSON.stringify({
      success: false,
      error: error.message,
      timestamp: new Date().toISOString()
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    })
  }
})

// Extract full content using Crawl4AI with fallback strategies
async function extractFullContent(url: string, description: string = ''): Promise<string> {
  console.log(`üï∑Ô∏è Extracting content from: ${url}`)
  
  // Strategy 1: Try Crawl4AI
  try {
    const crawl4aiUrl = Deno.env.get('CRAWL4AI_CLOUD_URL') || 'https://www.crawl4ai-cloud.com/query'
    const crawl4aiKey = Deno.env.get('CRAWL4AI_API_KEY')

    if (!crawl4aiKey) {
      console.warn('‚ö†Ô∏è CRAWL4AI_API_KEY not configured, falling back to alternative methods')
      throw new Error('CRAWL4AI_API_KEY not configured')
    }

    console.log('üì° Calling Crawl4AI API...')
    const response = await fetch(crawl4aiUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${crawl4aiKey}`
      },
      body: JSON.stringify({
        url: url,
        extract_main_content: true,
        remove_ads: true,
        word_count_threshold: 50
      })
    })

    if (!response.ok) {
      console.error(`‚ùå Crawl4AI API error: ${response.status} ${response.statusText}`)
      throw new Error(`Crawl4AI API error: ${response.status}`)
    }

    const result = await response.json()
    const content = result.data?.main_content || result.data?.text || result.data?.markdown || ''
    
    if (content && content.length > 100) {
      console.log(`‚úÖ Crawl4AI success: ${content.length} characters extracted`)
      return content
    } else {
      console.warn('‚ö†Ô∏è Crawl4AI returned minimal content, trying fallback')
      throw new Error('Insufficient content from Crawl4AI')
    }
    
  } catch (error) {
    console.error('‚ùå Crawl4AI extraction failed:', error.message)
  }

  // Strategy 2: Try Jina AI Reader as fallback
  try {
    console.log('üîÑ Fallback: Trying Jina AI Reader...')
    const jinaUrl = `https://r.jina.ai/${encodeURIComponent(url)}`
    
    const jinaResponse = await fetch(jinaUrl, {
      method: 'GET',
      headers: {
        'Accept': 'application/json',
        'User-Agent': 'KUATO-Intelligence/1.0'
      }
    })

    if (jinaResponse.ok) {
      const jinaData = await jinaResponse.json()
      
      if (jinaData.code === 200 && jinaData.data && jinaData.data.content) {
        const content = jinaData.data.content
        if (content && content.length > 100) {
          console.log(`‚úÖ Jina AI success: ${content.length} characters extracted`)
          console.log(`üìÑ Title: ${jinaData.data.title || 'No title'}`)
          
          // ÊèêÂèñÂõæÁâá‰ø°ÊÅØ
          let imageInfo = ''
          if (jinaData.data.images && jinaData.data.images.length > 0) {
            console.log(`üñºÔ∏è Found ${jinaData.data.images.length} images`)
            imageInfo = '\n\n[ÊñáÁ´†ÂõæÁâá‰ø°ÊÅØ]:\n' + jinaData.data.images.map((img: any, index: number) => 
              `ÂõæÁâá${index + 1}: ${img.src || img.url || img} ${img.alt ? `(${img.alt})` : ''}`
            ).join('\n')
          }
          
          return content + imageInfo
        }
      } else {
        console.warn(`‚ö†Ô∏è Jina AI returned code ${jinaData.code}: ${jinaData.status}`)
      }
    } else {
      console.error(`‚ùå Jina AI HTTP error: ${jinaResponse.status} ${jinaResponse.statusText}`)
    }
  } catch (error) {
    console.error('‚ùå Jina AI fallback failed:', error.message)
  }

  // Strategy 3: Simple fetch fallback
  try {
    console.log('üîÑ Final fallback: Simple fetch...')
    const simpleResponse = await fetch(url, {
      headers: {
        'User-Agent': 'Mozilla/5.0 (compatible; IntelligenceBot/1.0)'
      }
    })

    if (simpleResponse.ok) {
      const html = await simpleResponse.text()
      // Basic HTML content extraction (remove tags)
      const textContent = html
        .replace(/<script[^>]*>.*?<\/script>/gis, '')
        .replace(/<style[^>]*>.*?<\/style>/gis, '')
        .replace(/<[^>]*>/g, ' ')
        .replace(/\s+/g, ' ')
        .trim()

      if (textContent && textContent.length > 200) {
        console.log(`‚úÖ Simple fetch success: ${textContent.length} characters extracted`)
        return textContent.substring(0, 5000) // Limit to 5K chars
      }
    }
  } catch (error) {
    console.error('‚ùå Simple fetch fallback failed:', error.message)
  }

  // Final fallback: Use RSS description
  console.warn('‚ö†Ô∏è All content extraction methods failed, using RSS description as fallback')
  return description || 'Content extraction failed'
}

// Analyze content with Gemini AI
async function analyzeWithGemini(article: ArticleForAnalysis, fullContent: string) {
  const geminiKey = Deno.env.get('GEMINI_API_KEY')
  const geminiModel = Deno.env.get('GEMINI_MODEL') || 'gemini-1.5-flash'

  if (!geminiKey) {
    throw new Error('GEMINI_API_KEY not configured')
  }

  const analysisPrompt = `
# ‰∏Ä„ÄÅ Ê†∏ÂøÉ‰ΩøÂëΩ‰∏éÂÆö‰Ωç
‰Ω†ÁöÑË∫´‰ªΩÊòØÂêç‰∏∫„ÄåÂ¢ûÊùêÂà∂ÈÄ†Áãó„ÄçÁöÑ‰∫ß‰∏öÂ™í‰ΩìAIÂä©Êâã„ÄÇÊàë‰ª¨ÁöÑÂìÅÁâåÂÆö‰ΩçÊòØ‰∏Ä‰∏™**"Ê†πÂü∫ÊâéÂÆû„ÄÅËø≠‰ª£Á®≥Âø´ÁöÑÁü•ËØÜÁªìÊûÑ"**ÔºåÈù¢ÂêëÊú™Êù•ÁöÑÂÜ≥Á≠ñËÄÖÔºà‰ºÅ‰∏öÂÆ∂„ÄÅÊäïËµÑËÄÖ„ÄÅÊäÄÊúØË¥üË¥£‰∫∫Ôºâ„ÄÇ‰Ω†ÁöÑÊ†∏ÂøÉ‰ΩøÂëΩÊòØÔºö‰ªéÂÖ®ÁêÉÊäÄÊúØÂô™Èü≥‰∏≠ÔºåÈÄâÊã©Âπ∂Ëß£Á†ÅÈÇ£‰∫õÁúüÊ≠£ËÉΩÂºïÂèëË°å‰∏öÂèòÈù©ÁöÑ"‰ø°Âè∑"Ôºå‰∏∫Áî®Êà∑Êèê‰æõÊúÄÈ´òÂìÅÂë≥ÁöÑ"ÁªìÊûÑÂåñÊ¥ûÂØü"ÔºåÂ∏ÆÂä©‰ªñ‰ª¨‰ª•ÊúÄÈ´òÊïàÁéáËÆ§Áü•‰∏ñÁïåÔºåÁ®≥Âø´ÊâßË°å„ÄÇ

# ‰∫å„ÄÅ Ê†∏ÂøÉ‰ªªÂä°ÔºöÂàÜÊûêËØÑ‰º∞ + ÁîüÊàêÂø´ËÆØ
‰Ω†ÈúÄË¶ÅÂÆåÊàê‰∏§‰∏™‰ªªÂä°Ôºö
1. ËØÑ‰º∞ÊñáÁ´†ÁöÑÂ¢ûÊùêÂà∂ÈÄ†Áõ∏ÂÖ≥Â∫¶Âíå‰ª∑ÂÄº
2. Â¶ÇÊûúÁõ∏ÂÖ≥Â∫¶‚â•50ÂàÜÔºåÁîüÊàêÊ†áÂáÜÂåñ‰∏âÊÆµÂºèÂø´ËÆØ

# ‰∏â„ÄÅ Âø´ËÆØÊ†ºÂºèËßÑËåÉ (Strict Format Rules)
ÊØè‰∏ÄÁØá„ÄêÂø´ËÆØ„ÄëÈÉΩÂøÖÈ°ª‰∏•Ê†ºÈÅµÂæ™‰ª•‰∏ãÁªìÊûÑÂíåÊ†ºÂºèÔºå‰∏çÂæóÊúâ‰ªª‰ΩïÂÅèÂ∑Æ„ÄÇ

Ê†áÈ¢ò (Title):
Ê†ºÂºèÂøÖÈ°ª‰∏∫Ôºö „ÄêÂ¢ûÊùêÂà∂ÈÄ†Áãó„Äë- [‰∏§Â≠óÁ≥ªÂàó] - [ÊñáÁ´†Ê†áÈ¢ò]
[‰∏§Â≠óÁ≥ªÂàó]: ËøôÊòØÂØπÂÜÖÂÆπÁöÑÂàÜÁ±ªÔºå‰æãÂ¶ÇÔºöÂ∫îÁî®„ÄÅÂïÜ‰∏ö„ÄÅÁßëÁ†î„ÄÅÁîüÊÄÅ„ÄÅÂåªÁñó„ÄÅÂª∫Á≠ë„ÄÅÂÜõÊîø„ÄÅ‰∫ßÂìÅ„ÄÅÊùêÊñôÁ≠â„ÄÇ
[ÊñáÁ´†Ê†áÈ¢ò]: ÂøÖÈ°ªÁ≤æÁÇº„ÄÅÂáÜÁ°Æ„ÄÅÂπ∂ÂÖ∑ÊúâÂê∏ÂºïÂäõÔºåÊ¶ÇÊã¨Êñ∞ÈóªÊ†∏ÂøÉ„ÄÇ

‰ø°Ê∫êË°å (Source Line):
Ê†ºÂºèÂøÖÈ°ª‰∏∫Ôºö ‰ø°Ê∫êÔºö[Source Name] | ÁºñËØëÔºöÂ¢ûÊùêÂà∂ÈÄ†Áãó

Ê≠£Êñá (Body Text) - "‰∏âÊÆµÂºè"ÁªìÊûÑ:
Ê≠£ÊñáÂøÖÈ°ªÁî±‰∏â‰∏™Ëá™ÁÑ∂ÊÆµËêΩÁªÑÊàê„ÄÇ
ÊÆµËêΩ‰πãÈó¥ÂøÖÈ°ªÁî®‰∏Ä‰∏™Á©∫Ë°åÈöîÂºÄ„ÄÇ‰∏çÂæó‰ΩøÁî®‰ªª‰ΩïÂÖ∂‰ªñÂàÜÈöîÁ¨¶„ÄÇ
‰∏çÂæóÂú®ÊÆµËêΩÂâçÊ∑ªÂä†"ÂØºËØ≠"„ÄÅ"Ê†∏ÂøÉÂÜÖÂÆπ"„ÄÅ"ÁÆÄËØÑ"Á≠â‰ªª‰ΩïÊ†áÁ≠æ„ÄÇ

Á¨¨‰∏ÄÊÆµ (ÂØºËØ≠): ‰∫ãÂÆûÈôàËø∞‰∏éÂºïÂ≠ê„ÄÇÁî®‰∏ÄÂà∞‰∏§Âè•ËØùÔºåÊ∏ÖÊô∞„ÄÅÂáÜÁ°ÆÂú∞Ê¶ÇÊã¨Êñ∞ÈóªÁöÑÊ†∏ÂøÉ‰∫ã‰ª∂ÔºàË∞ÅÔºåÂÅö‰∫Ü‰ªÄ‰πàÔºåÂØºËá¥‰∫Ü‰ªÄ‰πàÔºâ„ÄÇÈ£éÊ†ºÔºöÂÆ¢ËßÇ„ÄÅÁõ¥Êé•ÔºåÂø´ÈÄüÂàáÂÖ•‰∏ªÈ¢ò„ÄÇ

Á¨¨‰∫åÊÆµ (Ê†∏ÂøÉÂÜÖÂÆπ): ÁªÜËäÇ‰∏éËÉåÊôØ„ÄÇÊèê‰æõÂÖ≥‰∫éËØ•Êñ∞Èóª‰∫ã‰ª∂ÁöÑÊõ¥Â§öÂÖ≥ÈîÆÁªÜËäÇ„ÄÅÊï∞ÊçÆÊàñËÉåÊôØ‰ø°ÊÅØÔºåËß£ÈáäÂÖ∂"Â¶Ç‰ΩïÂèëÁîü"‰ª•Âèä"ÂÖ∑‰ΩìÂÜÖÂÆπÊòØ‰ªÄ‰πà"„ÄÇÈ£éÊ†ºÔºö‰ø°ÊÅØÂØÜÈõÜÔºåÈÄªËæëÊ∏ÖÊô∞„ÄÇ

Á¨¨‰∏âÊÆµ (ÁÆÄËØÑ): Ê¥ûÂØü‰∏éËß£ËØª„ÄÇËøôÊòØ‰ΩìÁé∞Êàë‰ª¨‰ª∑ÂÄºÁöÑÊ†∏ÂøÉ„ÄÇÂàÜÊûêËøôÂàôÊñ∞ÈóªÁöÑÊÑè‰πâÔºåËß£ËØªÂÆÉÊòØ‰∏Ä‰∏™‰ªÄ‰πàÊ†∑ÁöÑ"‰ø°Âè∑"ÔºåÂÆÉÂ∞ÜÂØπË°å‰∏ö‰∫ßÁîü‰ªÄ‰πàÂΩ±ÂìçÔºåÊàñËÄÖÂÆÉ‰∏∫‰∏≠ÂõΩÁöÑ‰ªé‰∏öËÄÖÂ∏¶Êù•‰ªÄ‰πàÂêØÁ§∫„ÄÇÈ£éÊ†ºÔºöÁ≤æÁÇº„ÄÅÊïèÈîê„ÄÅÊúâËßÇÁÇπ„ÄÇ

ÂéüÂßã‰ø°Ê∫êÈìæÊé• (Original Source Link):
Ê†ºÂºèÂøÖÈ°ª‰∏∫Ôºö Âú®ÂÖ®ÊñáÁöÑÊúÄÂêéÔºåÂè¶Ëµ∑‰∏ÄË°åÔºå‰ª•"ÂéüÂßã‰ø°Ê∫êÈìæÊé•Ôºö"ÂºÄÂ§¥ÔºåÂπ∂Âú®‰∏ã‰∏ÄË°åÈôÑ‰∏äÂÆåÊï¥ÁöÑURL„ÄÇ

# Âõõ„ÄÅ ËæìÂá∫Ê†ºÂºè
ËØ∑‰∏•Ê†ºÊåâÁÖßJSONÊ†ºÂºèËæìÂá∫Ôºö

{
  "relevance_score": <0-100ËØÑÂàÜ>,
  "relevance_reason": "<ËØÑÂàÜÁêÜÁî±>",
  "primary_category": "<‰∏§Â≠óÁ≥ªÂàóÂàÜÁ±ªÔºöÂ∫îÁî®„ÄÅÂïÜ‰∏ö„ÄÅÁßëÁ†î„ÄÅÁîüÊÄÅ„ÄÅÂåªÁñó„ÄÅÂª∫Á≠ë„ÄÅÂÜõÊîø„ÄÅ‰∫ßÂìÅ„ÄÅÊùêÊñô„ÄÅÊ±ΩËΩ¶>",
  "entities": {
    "companies": ["<ÂÖ¨Âè∏ÂêçÁß∞>"],
    "technologies": ["<ÊäÄÊúØÂêçÁß∞>"], 
    "people": ["<‰∫∫Áâ©ÂêçÁß∞>"]
  },
  "summary_for_editor": "<200Â≠óÊëòË¶Å>",
  "strategic_implication": "<ÊàòÁï•ÊÑè‰πâÂàÜÊûê>",
  "compiled_briefing": "<Â¶ÇÊûúrelevance_score‚â•50ÔºåËæìÂá∫ÂÆåÊï¥‰∏âÊÆµÂºèÂø´ËÆØÔºõÂê¶Âàô‰∏∫null>"
}

# ‰∫î„ÄÅËØÑÂàÜÊ†áÂáÜ
- Áõ¥Êé•3DÊâìÂç∞Êñ∞ÈóªÔºàËÆæÂ§á„ÄÅÊäÄÊúØ„ÄÅÂ∫îÁî®ÔºâÔºö60-80ÂàÜ
- Áõ∏ÂÖ≥Âà∂ÈÄ†ÊäÄÊúØÊàñÊùêÊñôÔºö40-60ÂàÜ
- Èó¥Êé•Áõ∏ÂÖ≥Âà∂ÈÄ†‰∏öÔºö20-40ÂàÜ
- Êó†ÂÖ≥ÂÜÖÂÆπÔºö0-20ÂàÜ

Âä†ÂàÜÔºö‰∏≠ÂõΩÁõ∏ÂÖ≥+20%ÔºåÂ§ßËßÑÊ®°Áîü‰∫ß+15%ÔºåÊäÄÊúØÁ™ÅÁ†¥+10%

# ÂÖ≠„ÄÅÊñáÁ´†‰ø°ÊÅØ
- **ÊñáÁ´†Ê†áÈ¢ò**: ${article.title}
- **ÊñáÁ´†ÊèèËø∞**: ${article.description}
- **ÊñáÁ´†ÂÜÖÂÆπ**: ${fullContent || '(No full content available)'}
- **Êù•Ê∫êÈìæÊé•**: ${article.link}
- **‰ø°Ê∫êÂêçÁß∞**: ${article.rss_sources?.name || 'Unknown'}

ËØ∑ÂÆåÊàêÂàÜÊûêËØÑ‰º∞ÔºåÂ¶ÇÊûúÁõ∏ÂÖ≥Â∫¶‚â•50ÂàÜÂàôÂêåÊó∂ÁîüÊàêÊ†áÂáÜÂø´ËÆØ„ÄÇ
`

  const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${geminiModel}:generateContent?key=${geminiKey}`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      contents: [
        {
          parts: [
            {
              text: analysisPrompt
            }
          ]
        }
      ],
      generationConfig: {
        temperature: 0.1,
        topK: 1,
        maxOutputTokens: 2048,
      }
    })
  })

  if (!response.ok) {
    throw new Error(`Gemini API error: ${response.status}`)
  }

  const result = await response.json()
  const generatedText = result.candidates?.[0]?.content?.parts?.[0]?.text

  if (!generatedText) {
    throw new Error('No response from Gemini AI')
  }

  // Extract JSON from response
  const jsonMatch = generatedText.match(/\{[\s\S]*\}/)
  if (!jsonMatch) {
    throw new Error('Invalid JSON response from Gemini AI')
  }

  return JSON.parse(jsonMatch[0])
}

// Process and store entities
async function processEntities(supabase: any, articleId: string, entities: any) {
  const allEntities = [
    ...entities.companies?.map((name: string) => ({ name, type: 'company' })) || [],
    ...entities.technologies?.map((name: string) => ({ name, type: 'technology' })) || [],
    ...entities.people?.map((name: string) => ({ name, type: 'person' })) || []
  ]

  for (const entity of allEntities) {
    if (!entity.name || entity.name.trim().length < 2) continue

    const normalizedName = entity.name.toLowerCase().replace(/[^a-z0-9\u4e00-\u9fa5]/g, '_')
    
    // Upsert entity
    const { data: existingEntity } = await supabase
      .from('entities')
      .select('id')
      .eq('normalized_name', normalizedName)
      .single()

    let entityId = existingEntity?.id

    if (!entityId) {
      const { data: newEntity } = await supabase
        .from('entities')
        .insert({
          name: entity.name,
          normalized_name: normalizedName,
          type: entity.type,
          mention_count: 1,
          first_mentioned_at: new Date().toISOString(),
          last_mentioned_at: new Date().toISOString()
        })
        .select('id')
        .single()

      entityId = newEntity?.id
    } else {
      // Update mention count (get current count first, then increment)
      const { data: currentEntity } = await supabase
        .from('entities')
        .select('mention_count')
        .eq('id', entityId)
        .single()

      await supabase
        .from('entities')
        .update({
          mention_count: (currentEntity?.mention_count || 0) + 1,
          last_mentioned_at: new Date().toISOString()
        })
        .eq('id', entityId)
    }

    // Create article-entity relationship
    if (entityId) {
      await supabase
        .from('article_entities')
        .insert({
          article_id: articleId,
          entity_id: entityId,
          relevance_score: 1.0,
          extraction_method: 'ai'
        })
    }
  }
}